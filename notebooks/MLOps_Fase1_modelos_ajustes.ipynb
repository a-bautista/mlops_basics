{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zd0y1xaB3FBx",
    "outputId": "b9665165-9bf1-4d30-bed2-94876458d7af"
   },
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9RdvXLD8oZs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,make_scorer\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "#from imblearn.metrics import geometric_mean_score, classification_report_imbalanced\n",
    "from sklearn.model_selection import  cross_validate,  RepeatedStratifiedKFold,cross_val_score,GridSearchCV, train_test_split,cross_val_score\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score,make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07MkP0IsJG8j"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class function_fase_1:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    class load_data:\n",
    "        def __init__(self,filepath):\n",
    "            self.data, self.filepath=self.load_data(filepath)\n",
    "        def load_data(self,filepath):\n",
    "        # data (as pandas dataframes)\n",
    "            X = filepath.data.features\n",
    "            y = filepath.data.targets\n",
    "            # Merge them into a single DataFrame\n",
    "            data = X.copy()\n",
    "            data['Diabetes_binary'] = y\n",
    "            return data,filepath\n",
    "    \n",
    "    class explore_data:\n",
    "        def __init__(self,data,filepath):\n",
    "            self.data=data\n",
    "            self.filepath=filepath\n",
    "            self.explore_data(data,filepath)\n",
    "    \n",
    "        def explore_data(self,data, filepath):\n",
    "            print(filepath.metadata)  ## considerar\n",
    "            print(filepath.variables) ## considerar\n",
    "            print(data.head())\n",
    "            print(data.describe().T) ## considerar\n",
    "            print(data.info()) ## considerar\n",
    "            print(data.shape)\n",
    "            print(data.isnull().sum())\n",
    "            \n",
    "            # Automatically find binary columns\n",
    "            binary_columns = [col for col in data.columns if set(data[col].unique()).issubset({0, 1})]\n",
    "            \n",
    "            # Convert found binary columns to bool\n",
    "            for col in binary_columns:\n",
    "              data[col] = data[col].astype('bool')\n",
    "            \n",
    "            # List of categorical columns to convert\n",
    "            categorical_columns = ['GenHlth', 'Age', 'Education', 'Income']\n",
    "            \n",
    "            # Convert specified columns to category\n",
    "            for col in categorical_columns:\n",
    "              data[col] = data[col].astype('category')\n",
    "            \n",
    "            # Check the data types\n",
    "            print(data.dtypes)\n",
    "        \n",
    "    class plot_graph:\n",
    "    # 1. Summary Statistics\n",
    "        def __init__(self,data):\n",
    "            self.data=data\n",
    "            self.summary_statistics(data)\n",
    "            self.plot_numeric_distributions(data)\n",
    "            self.plot_binary_counts(data,'Diabetes_binary')\n",
    "            self.plot_boxplots(data,'Diabetes_binary')\n",
    "            self.plot_crosstab(data,'Diabetes_binary')\n",
    "            self.plot_correlation_heatmap(data)\n",
    "        \n",
    "        def summary_statistics(self,data):\n",
    "            print(\"Summary Statistics:\")\n",
    "            print(data.describe(include='all'))\n",
    "        \n",
    "        # 2. Distribution of Numeric Variables\n",
    "        def plot_numeric_distributions(self,data):\n",
    "            numeric_columns = data.select_dtypes(include=['int64']).columns\n",
    "            for col in numeric_columns:\n",
    "              plt.figure(figsize=(10, 5))\n",
    "              sns.histplot(data[col], bins=30, kde=True)\n",
    "              plt.title(f'Distribution of {col}')\n",
    "              plt.xlabel(col)\n",
    "              plt.ylabel('Frequency')\n",
    "              plt.grid()\n",
    "              plt.show()\n",
    "        \n",
    "        # 3. Count Plots for Binary Variables against the Target\n",
    "        def plot_binary_counts(self,data, target):\n",
    "            binary_columns = data.select_dtypes(include=['bool']).columns\n",
    "            for col in binary_columns:\n",
    "              plt.figure(figsize=(10, 5))\n",
    "              sns.countplot(x=data[col], hue=data[target])\n",
    "              plt.title(f'Count of {col} by {target}')\n",
    "              plt.xlabel(col)\n",
    "              plt.ylabel('Count')\n",
    "              plt.legend(title=target)\n",
    "              plt.grid()\n",
    "              plt.show()\n",
    "            \n",
    "        # 4. Box Plots for Continuous Variables by Target Variable\n",
    "        def plot_boxplots(self,data, target):\n",
    "            numeric_columns = data.select_dtypes(include=['int64']).columns\n",
    "            for num_col in numeric_columns:\n",
    "              plt.figure(figsize=(10, 5))\n",
    "              sns.boxplot(x=data[target], y=data[num_col])\n",
    "              plt.title(f'Boxplot of {num_col} by {target}')\n",
    "              plt.xlabel(target)\n",
    "              plt.ylabel(num_col)\n",
    "              plt.grid()\n",
    "              plt.show()\n",
    "        \n",
    "        # 5. Crosstabulation for Categorical Variables against the Target\n",
    "        def plot_crosstab(self,data, target):\n",
    "            categorical_columns = data.select_dtypes(include=['category']).columns\n",
    "            for cat_col in categorical_columns:\n",
    "              crosstab = pd.crosstab(data[cat_col], data[target])\n",
    "              print(f'Crosstab for {cat_col} vs {target}:')\n",
    "              print(crosstab)\n",
    "              sns.heatmap(crosstab, annot=True, fmt=\"d\", cmap='Blues')\n",
    "              plt.title(f'Crosstab Heatmap of {cat_col} by {target}')\n",
    "              plt.ylabel(cat_col)\n",
    "              plt.xlabel(target)\n",
    "              plt.show()\n",
    "        \n",
    "        # 6. Correlation Heatmap for Numeric Variables\n",
    "        def plot_correlation_heatmap(self,data):\n",
    "            numeric_data = data.select_dtypes(include=['int64'])\n",
    "            correlation_matrix = numeric_data.corr()\n",
    "            plt.figure(figsize=(15, 15))\n",
    "            sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "            plt.title('Correlation Heatmap')\n",
    "            plt.show()\n",
    "    \n",
    "    class transformation_type:\n",
    "        def __init__(self,data):\n",
    "            self.data=data\n",
    "            self.transformation_type(data)\n",
    "        \n",
    "        def transformation_type(self,data):\n",
    "        # Create a temporal DataFrame  for the transformations\n",
    "            temp_data = data.copy()\n",
    "            \n",
    "            # Create the transformations in the temporary DataFrame\n",
    "            temp_data['Log_BMI'] = np.log(temp_data['BMI'] + 1)\n",
    "            temp_data['Log_MentHlth'] = np.log(temp_data['MentHlth'] + 1)\n",
    "            temp_data['Log_PhysHlth'] = np.log(temp_data['PhysHlth'] + 1)\n",
    "            \n",
    "            temp_data['Sqrt_BMI'] = np.sqrt(temp_data['BMI'] + 1)\n",
    "            temp_data['Sqrt_MentHlth'] = np.sqrt(temp_data['MentHlth'] + 1)\n",
    "            temp_data['Sqrt_PhysHlth'] = np.sqrt(temp_data['PhysHlth'] + 1)\n",
    "            \n",
    "            # Yeo-Johnson Transformation\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "            temp_data[['YeoJohnson_BMI', 'YeoJohnson_MentHlth', 'YeoJohnson_PhysHlth']] = pt.fit_transform(temp_data[['BMI', 'MentHlth', 'PhysHlth']])\n",
    "            \n",
    "            # Configure the figure for multiple subgraphs\n",
    "            fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "            fig.suptitle('Comparison of Distributions of Numerical Variables and Transforms', fontsize=16)\n",
    "            \n",
    "            # Originals\n",
    "            for i, col in enumerate(['BMI', 'MentHlth', 'PhysHlth']):\n",
    "              sns.histplot(data[col], bins=30, kde=True, ax=axes[0, i])\n",
    "              axes[0, i].set_title(f'Distribución de {col}')\n",
    "              axes[0, i].set_xlabel(col)\n",
    "              axes[0, i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Logarithmic Transformation\n",
    "            for i, col in enumerate(['Log_BMI', 'Log_MentHlth', 'Log_PhysHlth']):\n",
    "              sns.histplot(temp_data[col], bins=30, kde=True, ax=axes[1, i])\n",
    "              axes[1, i].set_title(f'Distribución Log de {col}')\n",
    "              axes[1, i].set_xlabel(col)\n",
    "              axes[1, i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Square Root Transformation\n",
    "            for i, col in enumerate(['Sqrt_BMI', 'Sqrt_MentHlth', 'Sqrt_PhysHlth']):\n",
    "              sns.histplot(temp_data[col], bins=30, kde=True, ax=axes[2, i])\n",
    "              axes[2, i].set_title(f'Distribución Raíz Cuadrada de {col}')\n",
    "              axes[2, i].set_xlabel(col)\n",
    "              axes[2, i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Yeo-Johnson Transformation\n",
    "            for i, col in enumerate(['YeoJohnson_BMI', 'YeoJohnson_MentHlth', 'YeoJohnson_PhysHlth']):\n",
    "              sns.histplot(temp_data[col], bins=30, kde=True, ax=axes[3, i])\n",
    "              axes[3, i].set_title(f'Distribución Yeo-Johnson de {col}')\n",
    "              axes[3, i].set_xlabel(col)\n",
    "              axes[3, i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Occupying the space in the last row (you can adjust or add more graphs)\n",
    "            for ax in axes[4]:\n",
    "              ax.axis('off')  #Or you can add other graphs here\n",
    "            \n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])  #  Adjust design\n",
    "            plt.show()\n",
    "        \n",
    "    class transformation:\n",
    "        def __init__(self,data):\n",
    "            self.data=data\n",
    "            self.codification(data)\n",
    "            self.transform(data)\n",
    "        # 1. Codification\n",
    "        def codification(self,data):\n",
    "        # Inicializa el codificador\n",
    "            label_encoder = LabelEncoder()\n",
    "            \n",
    "            # Aplica el Label Encoding a las columnas ordinales\n",
    "            data['GenHlth'] = label_encoder.fit_transform(data['GenHlth'])\n",
    "            data['Age'] = label_encoder.fit_transform(data['Age'])\n",
    "            data['Education'] = label_encoder.fit_transform(data['Education'])\n",
    "            data['Income'] = label_encoder.fit_transform(data['Income'])\n",
    "            \n",
    "            # Muestra el DataFrame transformado\n",
    "            print(data[['GenHlth', 'Age', 'Education','Income']])\n",
    "            return data\n",
    "        \n",
    "        # 2. Transformation\n",
    "        def transform(self,data):\n",
    "        # Aplicar Yeo-Johnson y reemplazar las columnas originales\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "            data[['BMI', 'MentHlth', 'PhysHlth']] = pt.fit_transform(data[['BMI', 'MentHlth', 'PhysHlth']])\n",
    "            \n",
    "            # Estandarizar las variables transformadas y reemplazar las columnas originales\n",
    "            scaler = StandardScaler()\n",
    "            data[['BMI', 'MentHlth', 'PhysHlth']] = scaler.fit_transform(data[['BMI', 'MentHlth', 'PhysHlth']])\n",
    "            \n",
    "            # Mostrar el DataFrame transformado y estandarizado\n",
    "            print(data[['BMI', 'MentHlth', 'PhysHlth']])\n",
    "            return data\n",
    "    \n",
    "    class apply_PCA:\n",
    "        def __init__(self,data):\n",
    "            self.data=data\n",
    "            self.apply_PCA(data)\n",
    "            \n",
    "        def apply_PCA(self,data):\n",
    "        \n",
    "        # Seleccionar características para PCA (variables ya transformadas y escaladas)\n",
    "            X = data[['BMI', 'MentHlth', 'PhysHlth', 'GenHlth', 'Age', 'Education', 'Income']]\n",
    "            \n",
    "            # Inicializa PCA y ajusta el modelo\n",
    "            pca = PCA()\n",
    "            pca.fit(X)\n",
    "            \n",
    "            # Variancia explicada por cada componente\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            \n",
    "            # Crea un gráfico de codo\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "            plt.title('Varianza Explicada por Componentes Principales')\n",
    "            plt.xlabel('Número de Componentes Principales')\n",
    "            plt.ylabel('Proporción de Varianza Explicada')\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            # Crea un gráfico de varianza acumulada\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            cumulative_variance = explained_variance.cumsum()\n",
    "            plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "            plt.title('Varianza Acumulada por Componentes Principales')\n",
    "            plt.xlabel('Número de Componentes Principales')\n",
    "            plt.ylabel('Varianza Acumulada')\n",
    "            plt.grid()\n",
    "            plt.axhline(y=0.92, color='r', linestyle='--')  # Umbral del 92%\n",
    "            plt.show()\n",
    "            \n",
    "            # Seleccionar características para PCA (variables ya transformadas y escaladas)\n",
    "            X = data[['BMI', 'MentHlth', 'PhysHlth', 'GenHlth', 'Age', 'Education', 'Income']]\n",
    "            \n",
    "            # Aplicar PCA\n",
    "            pca = PCA(n_components=5)  # Elegir cuántas componentes principales mantener\n",
    "            X_pca = pca.fit_transform(X)\n",
    "            \n",
    "            # Convertir el resultado a un DataFrame\n",
    "            pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(5)])\n",
    "            \n",
    "            # Unir las componentes principales con el DataFrame original, excluyendo las variables originales\n",
    "            final_df = pd.concat([data.drop(columns=['BMI', 'MentHlth', 'PhysHlth', 'GenHlth', 'Age', 'Education', 'Income']).reset_index(drop=True), pca_df.reset_index(drop=True)], axis=1)\n",
    "            \n",
    "            # Mostrar el DataFrame final\n",
    "            print(final_df.head())\n",
    "            return final_df\n",
    "            \n",
    "                        \n",
    "                        # Run EDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_diabetes_health_indicators=fetch_ucirepo(id=891)\n",
    "main_function=function_fase_1()\n",
    "loading_data=main_function.load_data(cdc_diabetes_health_indicators)\n",
    "data,filepath=loading_data.data,loading_data.filepath\n",
    "exploration=main_function.explore_data(data,filepath)\n",
    "visualization=main_function.plot_graph(data)\n",
    "transformation_type=main_function.transformation_type(data)\n",
    "transformation=main_function.transformation(data)\n",
    "application_PCA=main_function.apply_PCA(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transf_to_model:\n",
    "    def __init___(self,df):\n",
    "        self.df_converted=self.true_false_to_one_hot(df)\n",
    "        \n",
    "    def true_false_to_one_hot(self,df):\n",
    "        df_converted=df.applymap(lambda x: 1 if x is True else (0 if x is False else x))\n",
    "        return df_converted\n",
    "    \n",
    "class process_split_info:\n",
    "    def __init__(self,df,target):\n",
    "        self.df=df\n",
    "        self.target=target\n",
    "        self.X_train,self.X_val,self.y_train,self.y_val=self.split_info(df,target)\n",
    "    def split_info(self,df,target):\n",
    "        return train_test_split(df.drop(target, axis='columns'), df[target], train_size=0.80, random_state=10,stratify=df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf=transf_to_model()\n",
    "df_converted=transf.true_false_to_one_hot(data)\n",
    "split=process_split_info(df_converted[:1000],'Diabetes_binary')\n",
    "X_train, X_val, y_train, y_val=split.X_train, split.X_val, split.y_train,split.y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytv = df_converted['Diabetes_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clases no balanceadas: tengo un 13% de pacientes con diabetes diagnosticado. Necesito un modelo con clases no balanceadas y luego otro modelo donde las clases estén balanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytv.sum()/ytv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    class model_statistics:\n",
    "        def __init__(self,yreal,ypred):\n",
    "            self.yreal = yreal\n",
    "            self.ypred = ypred\n",
    "            self.my_accuracy(yreal, ypred)\n",
    "            self.my_recall(yreal, ypred)\n",
    "            self.my_gmean(yreal, ypred)\n",
    "            self.my_precision(yreal, ypred)\n",
    "            self.my_f1_score(yreal, ypred)\n",
    "            self.mi_cm(yreal, ypred)\n",
    "    \n",
    "        def my_accuracy(self, yreal, ypred):\n",
    "            vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "            tot = confusion_matrix(yreal, ypred).sum()\n",
    "            return (vp + vn) / tot\n",
    "        \n",
    "        def my_recall(self, yreal, ypred):\n",
    "            vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "            return vp / (vp + fn)\n",
    "        \n",
    "        def my_gmean(self, yreal, ypred):\n",
    "            vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "            especificidad = vn / (vn + fp)\n",
    "            recall = self.my_recall(yreal, ypred)  # Cambia a self\n",
    "            return np.sqrt(recall * especificidad)\n",
    "        \n",
    "        def my_precision(self, yreal, ypred):\n",
    "            vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "            return vp / (vp + fp)\n",
    "        \n",
    "        def my_f1_score(self, yreal, ypred):\n",
    "            vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "            return (2 * vp) / (2 * vp + fp + fn)\n",
    "    \n",
    "        def mi_cm(self, yreal, ypred):\n",
    "            cm = confusion_matrix(yreal, ypred)\n",
    "            text = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
    "            vf = ['( TN )', '( FP )', '( FN )', '( TP )']\n",
    "            freq = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "            percent = [\"{0:.1%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
    "            \n",
    "            labels = [f\"{v1}\\n{v2}\\n{v3}\\n{v4}\" for v1, v2, v3, v4 in zip(text, vf, freq, percent)]\n",
    "            labels = np.asarray(labels).reshape(2, 2)\n",
    "            \n",
    "            plt.figure(figsize=(6, 4))\n",
    "            ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Spectral', cbar=False)\n",
    "            ax.set(ylabel=\"Real labels\", xlabel=\"Prediction labels\")\n",
    "            plt.show()\n",
    "    class list_models:\n",
    "        def __init__(self):\n",
    "            self.instance_uo, self.modelos, self.nombres = self.get_models_underoversampling()\n",
    "            self.creation_models()\n",
    "        \n",
    "        def get_models_underoversampling(self):\n",
    "            instance_uo, modelos, nombres = list(), list(), list()\n",
    "            dict_uo = {1: RandomOverSampler(), 2: TomekLinks(), 3: SMOTE(), 4: SMOTEENN()}\n",
    "            dict_nombres = {0: 'Log', 1: 'Log_RandOver', 2: 'Log_TomekLinks', 3: 'Log_SMOTE', 4: 'Log_SMOTEENN', 5: 'RandForest'}\n",
    "            for i in range(6):\n",
    "                if i in [0, 5]:\n",
    "                    instance_uo.append(np.NaN)\n",
    "                else:\n",
    "                    instance_uo.append(dict_uo.get(i))\n",
    "                if i == 0:\n",
    "                    modelos.append(LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "                elif i == 5:\n",
    "                    modelos.append(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "                else:\n",
    "                    modelos.append(LogisticRegression(class_weight='balanced', n_jobs=-1,solver='lbfgs', max_iter=1000))\n",
    "                nombres.append(dict_nombres.get(i))\n",
    "            return instance_uo, modelos, nombres\n",
    "        \n",
    "        def creation_models(self):\n",
    "            for inst_uo, model, name in zip(self.instance_uo, self.modelos, self.nombres):\n",
    "                resultados=[]\n",
    "                if type(inst_uo)==float:\n",
    "                    model_pipeline=make_pipeline(model)   \n",
    "                else:\n",
    "                    model_pipeline = make_pipeline((inst_uo),(model))\n",
    "                \n",
    "                metrics = {'accuracy','recall','f1'}\n",
    "                kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3)\n",
    "                resultadosOU = cross_validate(model_pipeline, X_train, y_train, scoring=metrics, cv=kfold)\n",
    "                resultados.append(resultadosOU)\n",
    "                \n",
    "                print('%s:\\nmean Accuracy: %.3f (%.4f)\\nmean Recall: %.3f (%.4f)\\nmean F1_score: %.3f (%.4f)\\n' % (name,\n",
    "                                                                                          np.mean(resultadosOU['test_accuracy']),\n",
    "                                                                                          np.std(resultadosOU['test_accuracy']), \n",
    "                                                                                          np.mean(resultadosOU['test_recall']),\n",
    "                                                                                          np.std(resultadosOU['test_recall']),\n",
    "                                                                                          np.mean(resultadosOU['test_f1']),\n",
    "                                                                                          np.std(resultadosOU['test_f1']),  \n",
    "                                                                                          ))\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process=process_model()\n",
    "process.list_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming X is the feature set and y is the target variable\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_converted.drop('Diabetes_binary', axis='columns'), df_converted['Diabetes_binary'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 8],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42,n_jobs=-1)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with the best parameters: {accuracy * 100:.2f}%\")\n",
    "print(f\"Recall of Random Forest model: {recall * 100:.2f}%\")\n",
    "print(f\"F1 of Random Forest model: {f1 * 100:.2f}%\")\n",
    "print(f\"Precision of Random Forest model: {precision * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class process_model:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     class model_statistics:\n",
    "#         def __init__(self,yreal,ypred):\n",
    "#             self.yreal = yreal\n",
    "#             self.ypred = ypred\n",
    "#             self.my_accuracy(yreal, ypred)\n",
    "#             self.my_recall(yreal, ypred)\n",
    "#             self.my_gmean(yreal, ypred)\n",
    "#             self.my_precision(yreal, ypred)\n",
    "#             self.my_f1_score(yreal, ypred)\n",
    "#             self.mi_cm(yreal, ypred)\n",
    "    \n",
    "#         def my_accuracy(self, yreal, ypred):\n",
    "#             vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "#             tot = confusion_matrix(yreal, ypred).sum()\n",
    "#             return (vp + vn) / tot\n",
    "        \n",
    "#         def my_recall(self, yreal, ypred):\n",
    "#             vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "#             return vp / (vp + fn)\n",
    "        \n",
    "#         def my_gmean(self, yreal, ypred):\n",
    "#             vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "#             especificidad = vn / (vn + fp)\n",
    "#             recall = self.my_recall(yreal, ypred)  # Cambia a self\n",
    "#             return np.sqrt(recall * especificidad)\n",
    "        \n",
    "#         def my_precision(self, yreal, ypred):\n",
    "#             vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "#             return vp / (vp + fp)\n",
    "        \n",
    "#         def my_f1_score(self, yreal, ypred):\n",
    "#             vn, fp, fn, vp = confusion_matrix(yreal, ypred).ravel()\n",
    "#             return (2 * vp) / (2 * vp + fp + fn)\n",
    "    \n",
    "#         def mi_cm(self, yreal, ypred):\n",
    "#             cm = confusion_matrix(yreal, ypred)\n",
    "#             text = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
    "#             vf = ['( TN )', '( FP )', '( FN )', '( TP )']\n",
    "#             freq = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "#             percent = [\"{0:.1%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
    "            \n",
    "#             labels = [f\"{v1}\\n{v2}\\n{v3}\\n{v4}\" for v1, v2, v3, v4 in zip(text, vf, freq, percent)]\n",
    "#             labels = np.asarray(labels).reshape(2, 2)\n",
    "            \n",
    "#             plt.figure(figsize=(6, 4))\n",
    "#             ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Spectral', cbar=False)\n",
    "#             ax.set(ylabel=\"Real labels\", xlabel=\"Prediction labels\")\n",
    "#             plt.show()\n",
    "#     class list_models:\n",
    "#         def __init__(self):\n",
    "#             self.instance_uo, self.modelos, self.nombres = self.get_models_underoversampling()\n",
    "#             self.creation_models()\n",
    "        \n",
    "#         def get_models_underoversampling(self):\n",
    "#             instance_uo, modelos, nombres = list(), list(), list()\n",
    "#             dict_uo = {1: RandomOverSampler(), 2: TomekLinks(), 3: SMOTE(), 4: SMOTEENN()}\n",
    "#             dict_nombres = {0: 'Log', 1: 'Log_RandOver', 2: 'Log_TomekLinks', 3: 'Log_SMOTE', 4: 'Log_SMOTEENN', 5: 'RandForest'}\n",
    "#             for i in range(6):\n",
    "#                 if i in [0, 5]:\n",
    "#                     instance_uo.append(np.NaN)\n",
    "#                 else:\n",
    "#                     instance_uo.append(dict_uo.get(i))\n",
    "#                 if i == 0:\n",
    "#                     modelos.append(LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "#                 elif i == 5:\n",
    "#                     modelos.append(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "#                 else:\n",
    "#                     modelos.append(LogisticRegression(class_weight='balanced', n_jobs=-1,solver='lbfgs', max_iter=1000))\n",
    "#                 nombres.append(dict_nombres.get(i))\n",
    "#             return instance_uo, modelos, nombres\n",
    "        \n",
    "#         def creation_models(self):\n",
    "#             for inst_uo, model, name in zip(self.instance_uo, self.modelos, self.nombres):\n",
    "#                 resultados=[]\n",
    "#                 if type(inst_uo)==float:\n",
    "#                     model_pipeline=make_pipeline(model)   \n",
    "#                 else:\n",
    "#                     model_pipeline = make_pipeline((inst_uo),(model))\n",
    "                \n",
    "#                 metrics = {\n",
    "#                     'accuracy': make_scorer(process_model.model_statistics.my_accuracy), \n",
    "#                     'recall': make_scorer(process_model.model_statistics.my_recall),\n",
    "#                     'f1_score': make_scorer(process_model.model_statistics.my_f1_score)\n",
    "#                 }\n",
    "#                 kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3)\n",
    "#                 resultadosOU = cross_validate(model_pipeline, X_train, y_train, scoring=metrics, cv=kfold)\n",
    "#                 resultados.append(resultadosOU) \n",
    "#                 print('%s:\\nmean Accuracy: %.3f (%.4f)\\nmean Recall: %.3f (%.4f)\\nmean F1_score: %.3f (%.4f)\\n' % (name,\n",
    "#                                                                                           np.mean(resultadosOU['test_accuracy']),\n",
    "#                                                                                           np.std(resultadosOU['test_accuracy']), \n",
    "#                                                                                           np.mean(resultadosOU['test_recall']),\n",
    "#                                                                                           np.std(resultadosOU['test_recall']),\n",
    "#                                                                                           np.mean(resultadosOU['test_f1_score']),\n",
    "#                                                                                           np.std(resultadosOU['test_f1_score']),  \n",
    "#                                                                                           ))\n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
